{
  "name": "Hashbox",
  "tagline": "Hashbox/Hashback is a cross-platform content addressable server and backup client",
  "body": "```\r\n\t ,+---+    \r\n\t+---+´|    HASHBOX / HASHBACK\r\n\t| # | |    Copyright 2015-2016 Fredrik Lidström\r\n\t+---+´     \r\n```\r\n\r\n# Hashbox #\r\n\r\n[Download binaries](https://github.com/fredli74/hashbox/releases/latest) *(only binaries for platforms that I personally test and run)*\r\n\r\n**DISCLAIMER! This is beta version it should no be used for important production**\r\n\r\n[![Build Status](https://semaphoreci.com/api/v1/fredli74/hashbox/branches/master/badge.svg)](https://semaphoreci.com/fredli74/hashbox)\r\n\r\nHashbox is a cross-platform derivate of a proprietary backup system (called BUP) that Magnus Lidström invented back around 2001.\r\n\r\n## Key concepts ##\r\n* General purpose data block storage system (Hashbox)\r\n* Efficient backup system with full data de-duplication (Hashback)\r\n* Cross-platform GO open source with minimal external dependencies\r\n* Single binaries with no runtime dependencies\r\n\r\n## Hashbox Server ##\r\n* Hashbox blocks are variable in length\r\n* Each block can optionally be compressed (only zlib implemented)\r\n* Each block has a unique 128 bit ID / hash calculated from the plain uncompressed block data and references\r\n* Each block can refer to other blocks (used for GC)\r\n* The server keeps a simple local database of accounts\r\n* The server keeps a pure transactional database of datasets and dataset versions for each account\r\n* Datasets contains a name, version and a reference to a \"root\" block ID\r\n* Everything not referenced by a dataset are subject to GC\r\n\r\n### Starting the server ###\r\n\r\n**Create a user**\r\n\r\n`./hashbox-freebsd-amd64 adduser <username> <password>`\r\n\r\n\r\n**Start the server**\r\n\r\n`./hashbox-freebsd-amd64 [-port=<port>] [-data=<path>] [-index=<path>]`\r\n\r\nOptional arguments data and index will tell the server where to keep all data and index/metadata files. If possible the index path should be put on fast storage such as SSD as they are used for every single access. These files can also be recreated from the data files by running the -repair command.\r\n\r\n\r\n**Run a garbage collect (GC)**\r\n\r\n`./hashbox-freebsd-amd64 gc [-compact]`\r\n\r\nOptional argument compact will run the compact phase on data and meta files freeing up unused space. After the sweep has completed, a list of how much unused data (dead data) there is in each storage file will be displayed. Running without the compact option will only do mark and sweep on the index files.\r\n\r\n\r\n**Run a storage file check**\r\n\r\n`./hashbox-freebsd-amd64 check-storage [-repair] [-skipdata] [-skipmeta] [-skipindex]`\r\n\r\nOptional arguments skipdata, skipmeta and skipindex can skip checks to speed up the procedure. If repair is specified it will recreate metadata and indexes from blocks found in data files. Bare in mind that doing this after an index GC will re-add the indexes as alive blocks again.\r\n\r\n\r\n## Hashbox Backup Client (Hashback) ##\r\n* Each file is split into blocks based on a rolling checksum\r\n* Each block hash (block ID) is calculated and sent to the server\r\n* Server requests only blocks that it does not already have, this in combination with the rollsum splitting allows the server to only request parts of files that were not previously stored\r\n* File metadata such as file name, size, modification time and attributes are stored in a directory block\r\n* A tree of directory blocks are then saved as a dataset version to the server\r\n* During backup a full file list is saved locally in a cache file. This file is used for reference during the next incremental backup. This allows for fast file skipping based on file name, size and date.\r\n* Incremental backups are done by always checking the root block ID of the last backup. If this is the same as the local cache file, the cache file will be used, otherwise the full file reference list is downloaded from the server.\r\n* A standard set of platform-specific files to ignore is included, addtional files to ignore can be added with the -ignore option\r\n* Optional retention of old backups allows you to keep weekly and daily backups for a specified duration (backups made the past 24 hours are always kept)\r\n\r\n### Using the client ###\r\n\r\n**Setup connection options and save them as default**\r\n\r\n`./hashback -user=<username> -password=<password> -server=<ip>:<port> -progress -saveoptions`\r\n\r\n\r\n**Add installation specific files to ignore**\r\n\r\n`hashback.exe -ignore=D:\\temp -showoptions -saveoptions`\r\n\r\nIgnore is case sensitive (even on Windows platform) so make sure it matches with the local files. Ignore pattern can contain `*` to match any number of characters, `?` to match one character or `[a-z]` to match a range of characters. An ignore pattern ending with a path separator will only match directories.\r\n\r\n\r\n**Show account information**\r\n\r\n`./hashback info` \r\n\r\n\r\n**Create a backup**\r\n\r\n`./hashback -retaindays=7 -retainweeks=10 store <dataset> (<folder> | <file>)...`\r\n\r\nIn the example above, all backups for the past 24 hours will be kept, 1 backup per day for 7 days and 1 backup per week for 10 weeks. Everything else will be removed after a successfull backup.\r\n\r\n\r\n**Run continuous backup**\r\n\r\n`./hashback -interval=60 store <dataset> (<folder> | <file>)...`\r\n\r\nIn the example above, a new backup will be made every 60 minutes. If an error occurs during backup (even a disconnect), hashback will exit with an error code.\r\n\r\n\r\n**Show a list of datasets or list files inside a dataset**\r\n\r\n`./hashback list <dataset> [(<backup id>|.) [\"<path>\"]]`\r\n\r\nIf `.` is used as backup id then the last backup will be listed (or restored)\r\n\r\n\r\n**Restore a file**\r\n\r\n`./hashback restore <dataset> (<backup id>|.) [\"<path>\"...] <dest-folder>`\r\n\r\n\r\n**Run a filediff to compare local files to stored files**\r\n\r\n`./hashback diff <dataset> (<backup id>|.) [\"<path>\"...] <local-folder>`\r\n\r\n\r\n**Manually remove a backup id from a dataset**\r\n\r\n`./hashback remove <dataset> <backup id>`\r\n\r\nIf the last backup id of a dataset is removed, the dataset will no longer be listed\r\n\r\n\r\n### Roadmap ###\r\nThings that should be implemented or considered\r\n* Client should have a resume option on store by default (use local cache to figure out what has been backed up)\r\n* Server low storage space threshold, return error on store before hitting 0 free space\r\n* Client platform specific file information (User/Group on linux, system/hidden on Windows for example). Client should store a root block with platform information\r\n* Server admin interface (API?) to adduser and change password so it can be done online\r\n* Server GC scheduled or triggered on low free space\r\n* Server GC mark and sweep partially or fully online\r\n* Server GC compact phase online through the storage engine\r\n* Server mirroring\r\n* Client GUI\r\n* Client data encryption (system design allows it)\r\n* Server quota calculations and restrictions (combine it with GC index mark phase?)",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}